import streamlit as st
import os
import json
import io
import tempfile
from pydub import AudioSegment
from google.cloud import speech
from google.oauth2 import service_account
from google import genai
from google.genai import types
from openai import OpenAI

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="AI èªéŸ³æ·¨åŒ–å™¨", page_icon="âœ¨")
st.title("âœ¨ AI èªéŸ³æƒ…ç·’æ·¨åŒ–å™¨")
st.markdown("è«‹é»æ“Šä¸‹æ–¹éº¥å…‹é¢¨éŒ„è£½ä¸€æ®µèªéŸ³ï¼ŒAI å°‡è‡ªå‹•æŠŠè² é¢è©å½™è®Šæˆç¾å¥½çš„è©èªã€‚")

# --- API è¨­å®šèˆ‡ Client åˆå§‹åŒ– ---
def get_secret(key):
    if key in st.secrets:
        return st.secrets[key]
    if os.getenv(key):
        return os.getenv(key)
    return None

try:
    # Google Cloud æ†‘è­‰è™•ç†
    if "google_cloud" in st.secrets:
        creds_dict = dict(st.secrets["google_cloud"])
        creds = service_account.Credentials.from_service_account_info(creds_dict)
        speech_client = speech.SpeechClient(credentials=creds)
    else:
        speech_client = speech.SpeechClient() # æœ¬åœ°é–‹ç™¼ fallback

    # å…¶ä»– Clients
    google_api_key = get_secret("GOOGLE_API_KEY")
    openai_api_key = get_secret("OPENAI_API_KEY")
    
    if not google_api_key or not openai_api_key:
        st.error("æ‰¾ä¸åˆ° API é‡‘é‘°ï¼Œè«‹æª¢æŸ¥ Secrets è¨­å®šã€‚")
        st.stop()

    gemini_client = genai.Client(api_key=google_api_key)
    openai_client = OpenAI(api_key=openai_api_key)

except Exception as e:
    st.error(f"ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
    st.stop()

# --- è¼”åŠ©å‡½æ•¸ï¼šæ»‘å‹•è¦–çª—åŒ¹é… ---
def perform_sliding_window_match(asr_words: list, replacement_map: dict) -> list:
    final_logs = []
    i = 0
    n = len(asr_words)
    MAX_WINDOW_SIZE = 5

    while i < n:
        matched = False
        for window_size in range(min(MAX_WINDOW_SIZE, n - i), 0, -1):
            words_slice = asr_words[i : i + window_size]
            candidate_phrase = "".join([w['word'] for w in words_slice])
            
            if candidate_phrase in replacement_map:
                replacement_word = replacement_map[candidate_phrase]
                start_seconds = words_slice[0]['start_time'].total_seconds()
                end_seconds = words_slice[-1]['end_time'].total_seconds()
                
                duration = end_seconds - start_seconds + 1.5
                if duration <= 0: duration = 0.5
                
                speed_instruction = "normal"
                if duration < 0.4: speed_instruction = "fast"
                elif duration > 1.5: speed_instruction = "slow"

                final_logs.append({
                    "original_word": candidate_phrase,
                    "replacement": replacement_word,
                    "start_time": start_seconds, # ä¿æŒ float æ–¹ä¾¿å¾ŒçºŒè¨ˆç®—
                    "end_time": end_seconds,
                    "duration_seconds": duration,
                    "speed_prompt": speed_instruction
                })
                i += window_size
                matched = True
                break
        if not matched:
            i += 1
    return final_logs

# --- æ ¸å¿ƒä»‹é¢ï¼šéŒ„éŸ³åŠŸèƒ½ ---
audio_input = st.audio_input("é»æ“Šéº¥å…‹é¢¨é–‹å§‹éŒ„éŸ³")

if audio_input is not None:
    if st.button("ğŸš€ é–‹å§‹æ·¨åŒ–è½‰æ›", type="primary"):
        status_text = st.empty()
        progress_bar = st.progress(0)

        try:
            # Step 1: è®€å–éŒ„éŸ³èˆ‡ ASR
            status_text.text("æ­£åœ¨è†è½ä¸¦è­˜åˆ¥èªéŸ³ (ASR)...")
            progress_bar.progress(10)
            
            audio_input.seek(0)
            audio_bytes = audio_input.read() # è®€å–åŸå§‹ bytes ä¾› ASR å’Œ pydub ä½¿ç”¨
            
            audio = speech.RecognitionAudio(content=audio_bytes)
            
            # è¨­å®šç‚º UNSPECIFIED è®“ Google è‡ªå‹•åµæ¸¬ WAV æ ¼å¼
            config = speech.RecognitionConfig(
                encoding=speech.RecognitionConfig.AudioEncoding.ENCODING_UNSPECIFIED, 
                sample_rate_hertz=0, # è¨­ç‚º 0 æˆ–åˆªé™¤è©²è¡Œï¼Œè®“å…¶è‡ªå‹•åµæ¸¬
                language_code="zh-TW",
                enable_word_time_offsets=True,
                enable_automatic_punctuation=True,
            )

            operation = speech_client.recognize(config=config, audio=audio)
            
            if not operation.results:
                st.warning("æ²’æœ‰åµæ¸¬åˆ°æ¸…æ™°çš„èªéŸ³ï¼Œè«‹å†è©¦ä¸€æ¬¡ã€‚")
                st.stop()

            result = operation.results[0].alternatives[0]
            transcript = result.transcript
            
            asr_words_data = []
            for word_info in result.words:
                asr_words_data.append({
                    "word": word_info.word.strip(),
                    "start_time": word_info.start_time,
                    "end_time": word_info.end_time
                })
            
            st.info(f"è­˜åˆ¥åˆ°çš„å…§å®¹: {transcript}")
            progress_bar.progress(30)

            # Step 2: LLM åˆ¤æ–·
            status_text.text("AI æ­£åœ¨æ€è€ƒå¦‚ä½•è®“é€™å¥è©±æ›´ç¾å¥½...")
            
            schema = {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "original_word": {"type": "string"},
                        "replacement_word": {"type": "string"}
                    },
                    "required": ["original_word", "replacement_word"]
                }
            }
            
            prompt = f"""
            ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æƒ…ç·’è©å½™å¯©æŸ¥èˆ‡è½‰æ›å¼•æ“ã€‚
            ä»»å‹™ï¼šæ‰¾å‡ºè² é¢æƒ…ç·’è©å½™ä¸¦æ›¿æ›ç‚ºæ­£å‘ã€æ„è±¡ç¾å¥½çš„è©å½™ (å¦‚ï¼šå½©è™¹ã€èŠ±æœµã€æ³¡æ³¡ã€æ£‰èŠ±ç³–)ã€‚
            è¼¸å…¥æ–‡æœ¬: "{transcript}"
            """
            
            llm_response = gemini_client.models.generate_content(
                model='gemini-2.0-flash',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema=schema,
                )
            )
            censor_list = json.loads(llm_response.text)
            replacement_map = { item['original_word'].strip(): item['replacement_word'] for item in censor_list }
            
            if not replacement_map:
                st.success("é€™å¥è©±å¾ˆæ£’ï¼Œæ²’æœ‰ç™¼ç¾è² é¢è©å½™ï¼")
                progress_bar.progress(100)
                st.stop()
                
            progress_bar.progress(50)

            # Step 3: åŒ¹é…æ™‚é–“è»¸
            timeline_rules = perform_sliding_window_match(asr_words_data, replacement_map)
            
            with st.expander("æŸ¥çœ‹ AI æ›¿æ›é‚è¼¯ç´°ç¯€"):
                st.write(timeline_rules)

            # Step 4: TTS ç”Ÿæˆ & å…§éƒ¨æ··éŸ³
            status_text.text("æ­£åœ¨ç”Ÿæˆç”œç¾çš„è²éŸ³ä¸¦é€²è¡Œæ··éŸ³...")
            
            # ä½¿ç”¨ pydub è¼‰å…¥åŸå§‹éŸ³è¨Š
            # æ³¨æ„: st.audio_input ç”¢ç”Ÿçš„é€šå¸¸æ˜¯ wav
            try:
                original_audio = AudioSegment.from_file(io.BytesIO(audio_bytes))
            except Exception as e:
                # å¦‚æœç„¡æ³•è­˜åˆ¥ï¼Œå˜—è©¦å¼·åˆ¶æŒ‡å®š wav
                original_audio = AudioSegment.from_file(io.BytesIO(audio_bytes), format="wav")

            for rule in timeline_rules:
                # 4-1. ç”Ÿæˆ TTS
                speed = 1.0
                if rule['speed_prompt'] == 'fast': speed = 1.2
                elif rule['speed_prompt'] == 'slow': speed = 0.8
                
                tts_resp = openai_client.audio.speech.create(
                    model="tts-1", voice="nova", input=rule['replacement'], speed=speed
                )
                
                # å°‡ TTS mp3 è½‰ç‚º AudioSegment
                tts_audio = AudioSegment.from_file(io.BytesIO(tts_resp.content), format="mp3")
                
                # 4-2. è¨ˆç®—æ™‚é–“é» (ç§’è½‰æ¯«ç§’)
                start_ms = int(rule['start_time'] * 1000)
                end_ms = int(rule['end_time'] * 1000)
                
                # 4-3. æ··éŸ³é‚è¼¯: 
                # (A) å°‡åŸå§‹éŸ³è¨Šä¸­ã€Œè² é¢è©ã€çš„ç‰‡æ®µéœéŸ³
                # (B) åœ¨è©²ä½ç½®ç–ŠåŠ æ–°çš„ TTS éŸ³è¨Š
                
                # ç‚ºäº†é¿å…é•·åº¦æ”¹è®Šå°è‡´å¾Œé¢çš„è²éŸ³å°ä¸ä¸Šï¼Œæˆ‘å€‘æ¡ç”¨ã€ŒéœéŸ³+ç–ŠåŠ ã€çš„æ–¹å¼
                # é€™æ¨£ç¸½æ™‚é•·ä¸è®Š
                
                # è£½ä½œä¸€æ®µéœéŸ³ï¼Œé•·åº¦ç­‰æ–¼åŸæœ¬çš„è² é¢è©é•·åº¦
                silence_duration = end_ms - start_ms
                if silence_duration < 0: silence_duration = 0
                silence_segment = AudioSegment.silent(duration=silence_duration)
                
                # æ›¿æ›åŸå§‹å€æ®µç‚ºéœéŸ³ (ä¿æŒé•·åº¦ä¸è®Š)
                original_audio = original_audio[:start_ms] + silence_segment + original_audio[end_ms:]
                
                # ç–ŠåŠ  TTS (position è¨­å®šåœ¨é–‹å§‹æ™‚é–“)
                # æ³¨æ„ï¼šå¦‚æœ TTS æ¯”åŸè©é•·ï¼Œæœƒè“‹åˆ°å¾Œé¢çš„å­—ï¼›å¦‚æœæ¯”è¼ƒçŸ­ï¼Œæœƒæœ‰ç•™ç™½ã€‚é€™æ˜¯æ­£å¸¸çš„ã€‚
                original_audio = original_audio.overlay(tts_audio, position=start_ms)

            progress_bar.progress(90)
            status_text.text("è™•ç†å®Œæˆï¼Œæ­£åœ¨è¼¸å‡º...")

            # åŒ¯å‡ºæœ€çµ‚æª”æ¡ˆ
            buffer = io.BytesIO()
            original_audio.export(buffer, format="mp3")
            final_audio_bytes = buffer.getvalue()

            progress_bar.progress(100)
            status_text.text("âœ¨ å®Œæˆï¼")
            st.balloons()
            
            st.subheader("ğŸ§ æ‚¨çš„æ·¨åŒ–ç‰ˆèªéŸ³")
            st.audio(final_audio_bytes, format='audio/mpeg')
            
            st.download_button(
                label="ä¸‹è¼‰ MP3",
                data=final_audio_bytes,
                file_name="censored_recording.mp3",
                mime="audio/mpeg"
            )

        except Exception as e:
            st.error(f"ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            import traceback
            st.code(traceback.format_exc())
