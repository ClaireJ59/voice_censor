import streamlit as st
import os
import json
import io
import shutil
import math
from pydub import AudioSegment
from google.cloud import speech
from google.oauth2 import service_account
from google import genai
from google.genai import types
from openai import OpenAI

# --- å¼·åˆ¶è¨­å®š FFmpeg è·¯å¾‘ (è§£æ±º iOS è½‰æª”å´©æ½°å•é¡Œ) ---
# Streamlit Cloud (Debian) çš„ ffmpeg é€šå¸¸åœ¨ /usr/bin/ffmpeg
ffmpeg_path = shutil.which("ffmpeg")
if ffmpeg_path:
    AudioSegment.converter = ffmpeg_path
    AudioSegment.ffmpeg = ffmpeg_path
    AudioSegment.ffprobe = shutil.which("ffprobe")
else:
    # å‚™ç”¨è·¯å¾‘
    AudioSegment.converter = "/usr/bin/ffmpeg" 

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="AI èªéŸ³æ·¨åŒ–å™¨ (iOS ç©©å®šç‰ˆ)", page_icon="ğŸ¤")
st.title("ğŸ¤ AI èªéŸ³æ·¨åŒ–å™¨")
st.markdown("æ”¯æ´ iOS/Android/PCï¼Œè‡ªå‹•å°‡è² é¢è©å½™è½‰æ›ç‚ºç¾å¥½æ„è±¡ã€‚")

# --- å´é‚Šæ¬„è¨­å®š ---
with st.sidebar:
    st.header("ğŸ›ï¸ æ··éŸ³è¨­å®š")
    manual_delay_ms = st.slider("æ‰‹å‹•å»¶é²ä¿®æ­£ (ms)", -500, 500, 0, 10)
    volume_boost = st.slider("æ›¿æ›éŸ³é‡ (dB)", 0, 30, 15)

# --- API åˆå§‹åŒ– ---
def get_secret(key):
    if key in st.secrets:
        return st.secrets[key]
    if os.getenv(key):
        return os.getenv(key)
    return None

try:
    if "google_cloud" in st.secrets:
        creds_dict = dict(st.secrets["google_cloud"])
        creds = service_account.Credentials.from_service_account_info(creds_dict)
        speech_client = speech.SpeechClient(credentials=creds)
    else:
        speech_client = speech.SpeechClient()

    google_api_key = get_secret("GOOGLE_API_KEY")
    openai_api_key = get_secret("OPENAI_API_KEY")
    
    if not google_api_key or not openai_api_key:
        st.error("é‡‘é‘°ç¼ºå¤±ï¼Œè«‹æª¢æŸ¥ Secretsã€‚")
        st.stop()

    gemini_client = genai.Client(api_key=google_api_key)
    openai_client = OpenAI(api_key=openai_api_key)

except Exception as e:
    st.error(f"åˆå§‹åŒ–éŒ¯èª¤: {e}")
    st.stop()

# --- Helper Functions ---
def speed_change(sound, speed=1.0):
    sound_with_altered_frame_rate = sound._spawn(sound.raw_data, overrides={
        "frame_rate": int(sound.frame_rate * speed)
    })
    return sound_with_altered_frame_rate.set_frame_rate(sound.frame_rate)

def perform_sliding_window_match(asr_words: list, replacement_map: dict) -> list:
    final_logs = []
    i = 0
    n = len(asr_words)
    MAX_WINDOW_SIZE = 5
    while i < n:
        matched = False
        for window_size in range(min(MAX_WINDOW_SIZE, n - i), 0, -1):
            words_slice = asr_words[i : i + window_size]
            candidate_phrase = "".join([w['word'] for w in words_slice])
            if candidate_phrase in replacement_map:
                replacement_word = replacement_map[candidate_phrase]
                start_seconds = words_slice[0]['start_time'].total_seconds()
                end_seconds = words_slice[-1]['end_time'].total_seconds()
                final_logs.append({
                    "original_word": candidate_phrase,
                    "replacement": replacement_word,
                    "start_time": start_seconds,
                    "end_time": end_seconds,
                    "duration_seconds": end_seconds - start_seconds,
                    "speed_prompt": "normal"
                })
                i += window_size
                matched = True
                break
        if not matched: i += 1
    return final_logs

# --- ä¸»ä»‹é¢èˆ‡é‚è¼¯ ---
audio_input = st.audio_input("è«‹æŒ‰éº¥å…‹é¢¨éŒ„éŸ³ (iOS è«‹ç¨ç­‰å¹¾ç§’ä¸Šå‚³)")

if audio_input is not None:
    # 1. å…ˆé¡¯ç¤ºéŒ„éŸ³æª”æ¡ˆè³‡è¨Šï¼Œç¢ºèª App æ²’æœ‰å´©æ½°
    audio_input.seek(0, os.SEEK_END)
    file_size = audio_input.tell()
    audio_input.seek(0)
    
    st.info(f"âœ… éŒ„éŸ³æˆåŠŸï¼æª”æ¡ˆå¤§å°: {file_size / 1024:.1f} KB")
    
    if st.button("ğŸš€ é–‹å§‹æ·¨åŒ–", type="primary"):
        status = st.status("æ­£åœ¨è™•ç†ä¸­...", expanded=True)
        
        try:
            # --- Step 1: æ ¼å¼è½‰æ› (æœ€å®¹æ˜“å‡ºéŒ¯çš„åœ°æ–¹) ---
            status.write("ğŸ”„ æ­£åœ¨è½‰æ›éŸ³è¨Šæ ¼å¼ (WAV)...")
            raw_bytes = audio_input.read()
            
            try:
                # å˜—è©¦è®€å– (è‡ªå‹•åµæ¸¬æ ¼å¼ï¼ŒåŒ…å« m4a)
                input_audio = AudioSegment.from_file(io.BytesIO(raw_bytes))
                
                # å¼·åˆ¶è½‰ç‚º Google å–œæ­¡çš„æ ¼å¼ (Mono, 16kHz) æ¸›è¼•è² è¼‰
                input_audio = input_audio.set_channels(1).set_frame_rate(16000)
                
                wav_buffer = io.BytesIO()
                input_audio.export(wav_buffer, format="wav")
                clean_wav_bytes = wav_buffer.getvalue()
                
            except Exception as ffmpeg_err:
                status.update(label="æ ¼å¼è½‰æ›å¤±æ•—", state="error")
                st.error(f"ç„¡æ³•è®€å–éŒ„éŸ³æª”ï¼Œå¯èƒ½æ˜¯ FFmpeg æœªå®‰è£æˆ–æ ¼å¼ä¸æ”¯æ´ã€‚\nè©³ç´°éŒ¯èª¤: {ffmpeg_err}")
                st.stop()

            # --- Step 2: ASR ---
            status.write("ğŸ‘‚ æ­£åœ¨è­˜åˆ¥èªéŸ³ (ASR)...")
            audio = speech.RecognitionAudio(content=clean_wav_bytes)
            config = speech.RecognitionConfig(
                encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                sample_rate_hertz=16000,
                language_code="zh-TW",
                enable_word_time_offsets=True,
                enable_automatic_punctuation=True,
            )
            operation = speech_client.recognize(config=config, audio=audio)
            
            if not operation.results:
                status.update(label="è­˜åˆ¥å¤±æ•—", state="error")
                st.warning("æ²’æœ‰è½æ¸…æ¥šæ‚¨èªªçš„è©±ï¼Œè«‹å†è©¦ä¸€æ¬¡ã€‚")
                st.stop()

            transcript = operation.results[0].alternatives[0].transcript
            asr_words_data = []
            for w in operation.results[0].alternatives[0].words:
                asr_words_data.append({
                    "word": w.word.strip(),
                    "start_time": w.start_time,
                    "end_time": w.end_time
                })
            
            status.write(f"ğŸ“ è­˜åˆ¥å…§å®¹: {transcript}")

            # --- Step 3: LLM ---
            status.write("ğŸ¤– AI æ­£åœ¨å¯©æŸ¥èˆ‡æ›¿æ›...")
            prompt = f"""
            ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æƒ…ç·’è©å½™å¯©æŸ¥èˆ‡è½‰æ›å¼•æ“ã€‚
            ä»»å‹™ï¼šæ‰¾å‡ºè² é¢æƒ…ç·’è©å½™ä¸¦æ›¿æ›ç‚ºæ­£å‘ã€æ„è±¡ç¾å¥½çš„è©å½™ (å¦‚ï¼šå½©è™¹ã€èŠ±æœµã€æ³¡æ³¡ã€æ£‰èŠ±ç³–)ã€‚
            è¼¸å…¥æ–‡æœ¬: "{transcript}"
            """
            # ç°¡åŒ– Schema
            schema = {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {"original_word": {"type": "string"}, "replacement_word": {"type": "string"}},
                    "required": ["original_word", "replacement_word"]
                }
            }
            
            llm_res = gemini_client.models.generate_content(
                model='gemini-2.5-flash', contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json", response_schema=schema)
            )
            censor_list = json.loads(llm_res.text)
            replacement_map = { i['original_word'].strip(): i['replacement_word'] for i in censor_list }
            
            if not replacement_map:
                status.update(label="å®Œæˆ", state="complete")
                st.success("é€™å¥è©±å¾ˆæ£’ï¼Œæ²’æœ‰è² é¢è©å½™ï¼")
                st.stop()

            # --- Step 4: æ··éŸ³ ---
            status.write("ğŸ¹ æ­£åœ¨åˆæˆèˆ‡æ··éŸ³...")
            timeline = perform_sliding_window_match(asr_words_data, replacement_map)
            final_audio = input_audio # ä½¿ç”¨è½‰æª”å¾Œçš„ä¹¾æ·¨éŸ³è¨Šç•¶åŸºåº•

            for rule in timeline:
                tts_resp = openai_client.audio.speech.create(
                    model="tts-1", voice="nova", input=rule['replacement']
                )
                rep_audio = AudioSegment.from_file(io.BytesIO(tts_resp.content), format="mp3")
                
                # æ™‚é–“èˆ‡è®Šé€Ÿ
                orig_dur = rule['duration_seconds']
                cur_len = len(rep_audio) / 1000.0
                speed = cur_len / orig_dur if orig_dur > 0 else 1.0
                speed = max(0.8, min(speed, 1.2))
                
                adj_audio = speed_change(rep_audio, speed) + volume_boost
                
                # ç½®ä¸­è¨ˆç®—
                orig_center_ms = (rule['start_time'] + rule['end_time']) * 1000 / 2
                pos_ms = int(orig_center_ms)
                
                final_audio = final_audio.overlay(adj_audio, position=max(0, pos_ms))

            # --- è¼¸å‡º ---
            status.update(label="è™•ç†å®Œæˆï¼", state="complete")
            out_buffer = io.BytesIO()
            final_audio.export(out_buffer, format="mp3")
            
            st.subheader("ğŸ§ æ‚¨çš„æ·¨åŒ–ç‰ˆèªéŸ³")
            st.audio(out_buffer.getvalue(), format='audio/mpeg')
            st.download_button("ä¸‹è¼‰ MP3", out_buffer.getvalue(), "remix.mp3", "audio/mpeg")

        except Exception as e:
            status.update(label="ç™¼ç”ŸéŒ¯èª¤", state="error")
            st.error(f"åŸ·è¡Œå¤±æ•—: {str(e)}")
