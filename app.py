import streamlit as st
import os
import json
import io
import requests
import tempfile
from google.cloud import speech
from google.oauth2 import service_account
from google import genai
from google.genai import types
from openai import OpenAI

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="AI èªéŸ³æ·¨åŒ–å™¨", page_icon="âœ¨")
st.title("âœ¨ AI èªéŸ³æƒ…ç·’æ·¨åŒ–å™¨")
st.markdown("è«‹é»æ“Šä¸‹æ–¹éº¥å…‹é¢¨éŒ„è£½ä¸€æ®µèªéŸ³ï¼ŒAI å°‡è‡ªå‹•æŠŠè² é¢è©å½™è®Šæˆç¾å¥½çš„è©èªã€‚")

# --- API è¨­å®šèˆ‡ Client åˆå§‹åŒ– (ä¿æŒä¸è®Š) ---
def get_secret(key):
    if key in st.secrets:
        return st.secrets[key]
    if os.getenv(key):
        return os.getenv(key)
    return None

try:
    # Google Cloud æ†‘è­‰è™•ç†
    if "google_cloud" in st.secrets:
        creds_dict = dict(st.secrets["google_cloud"])
        creds = service_account.Credentials.from_service_account_info(creds_dict)
        speech_client = speech.SpeechClient(credentials=creds)
    else:
        speech_client = speech.SpeechClient() # æœ¬åœ°é–‹ç™¼ fallback

    # å…¶ä»– Clients
    google_api_key = get_secret("GOOGLE_API_KEY")
    openai_api_key = get_secret("OPENAI_API_KEY")
    
    if not google_api_key or not openai_api_key:
        st.error("æ‰¾ä¸åˆ° API é‡‘é‘°ï¼Œè«‹æª¢æŸ¥ Secrets è¨­å®šã€‚")
        st.stop()

    gemini_client = genai.Client(api_key=google_api_key)
    openai_client = OpenAI(api_key=openai_api_key)

except Exception as e:
    st.error(f"ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
    st.stop()

EXTERNAL_MIX_URL = "https://a67e4a6a0969.ngrok-free.app/mix"

# --- è¼”åŠ©å‡½æ•¸ï¼šæ»‘å‹•è¦–çª—åŒ¹é… (ä¿æŒä¸è®Š) ---
def perform_sliding_window_match(asr_words: list, replacement_map: dict) -> list:
    final_logs = []
    i = 0
    n = len(asr_words)
    MAX_WINDOW_SIZE = 5

    while i < n:
        matched = False
        for window_size in range(min(MAX_WINDOW_SIZE, n - i), 0, -1):
            words_slice = asr_words[i : i + window_size]
            candidate_phrase = "".join([w['word'] for w in words_slice])
            
            if candidate_phrase in replacement_map:
                replacement_word = replacement_map[candidate_phrase]
                start_seconds = words_slice[0]['start_time'].total_seconds()
                end_seconds = words_slice[-1]['end_time'].total_seconds()
                
                duration = end_seconds - start_seconds + 1.5
                if duration <= 0: duration = 0.5
                
                speed_instruction = "normal"
                if duration < 0.4: speed_instruction = "fast"
                elif duration > 1.5: speed_instruction = "slow"

                final_logs.append({
                    "original_word": candidate_phrase,
                    "replacement": replacement_word,
                    "start_time": f"{start_seconds}s",
                    "end_time": f"{end_seconds}s",
                    "duration_seconds": duration,
                    "speed_prompt": speed_instruction
                })
                i += window_size
                matched = True
                break
        if not matched:
            i += 1
    return final_logs

# ==========================================
#  ğŸ”¥ æ ¸å¿ƒä»‹é¢ä¿®æ”¹ï¼šåªä¿ç•™éŒ„éŸ³åŠŸèƒ½
# ==========================================

# ç›´æ¥é¡¯ç¤ºéŒ„éŸ³å…ƒä»¶
audio_input = st.audio_input("é»æ“Šéº¥å…‹é¢¨é–‹å§‹éŒ„éŸ³")

if audio_input is not None:
    # é€™è£¡é¡¯ç¤ºè™•ç†æŒ‰éˆ•
    if st.button("ğŸš€ é–‹å§‹æ·¨åŒ–è½‰æ›", type="primary"):
        status_text = st.empty()
        progress_bar = st.progress(0)

        try:
            # Step 1: è®€å–éŒ„éŸ³èˆ‡ ASR
            status_text.text("æ­£åœ¨è†è½ä¸¦è­˜åˆ¥èªéŸ³ (ASR)...")
            progress_bar.progress(10)
            
            # é‡è¦ï¼šå°‡æŒ‡æ¨™ç§»å›é–‹é ­ä¸¦è®€å–
            audio_input.seek(0)
            audio_content = audio_input.read()
            
            audio = speech.RecognitionAudio(content=audio_content)
            
            # é‡å°ç€è¦½å™¨éŒ„éŸ³ (WAV) å„ªåŒ–çš„è¨­å®š
            config = speech.RecognitionConfig(
                encoding=speech.RecognitionConfig.AudioEncoding.ENCODING_UNSPECIFIED, # è‡ªå‹•åµæ¸¬ WAV/WebM
                sample_rate_hertz=48000, 
                language_code="zh-TW",
                enable_word_time_offsets=True,
                enable_automatic_punctuation=True,
            )

            operation = speech_client.recognize(config=config, audio=audio)
            
            if not operation.results:
                st.warning("æ²’æœ‰åµæ¸¬åˆ°æ¸…æ™°çš„èªéŸ³ï¼Œè«‹å†è©¦ä¸€æ¬¡ã€‚")
                st.stop()

            result = operation.results[0].alternatives[0]
            transcript = result.transcript
            
            # æ•´ç† ASR æ•¸æ“š
            asr_words_data = []
            for word_info in result.words:
                asr_words_data.append({
                    "word": word_info.word.strip(),
                    "start_time": word_info.start_time,
                    "end_time": word_info.end_time
                })
            
            st.info(f"è­˜åˆ¥åˆ°çš„å…§å®¹: {transcript}")
            progress_bar.progress(30)

            # Step 2: LLM åˆ¤æ–· (Gemini)
            status_text.text("AI æ­£åœ¨æ€è€ƒå¦‚ä½•è®“é€™å¥è©±æ›´ç¾å¥½...")
            
            schema = {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "original_word": {"type": "string"},
                        "replacement_word": {"type": "string"}
                    },
                    "required": ["original_word", "replacement_word"]
                }
            }
            
            prompt = f"""
            ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æƒ…ç·’è©å½™å¯©æŸ¥èˆ‡è½‰æ›å¼•æ“ã€‚
            ä»»å‹™ï¼šæ‰¾å‡ºè² é¢æƒ…ç·’è©å½™ä¸¦æ›¿æ›ç‚ºæ­£å‘ã€æ„è±¡ç¾å¥½çš„è©å½™ (å¦‚ï¼šå½©è™¹ã€èŠ±æœµã€æ³¡æ³¡ã€æ£‰èŠ±ç³–)ã€‚
            è¼¸å…¥æ–‡æœ¬: "{transcript}"
            """
            
            llm_response = gemini_client.models.generate_content(
                model='gemini-2.0-flash',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema=schema,
                )
            )
            censor_list = json.loads(llm_response.text)
            replacement_map = { item['original_word'].strip(): item['replacement_word'] for item in censor_list }
            
            if not replacement_map:
                st.success("é€™å¥è©±å¾ˆæ£’ï¼Œæ²’æœ‰ç™¼ç¾è² é¢è©å½™ï¼")
                progress_bar.progress(100)
                st.stop()
                
            progress_bar.progress(50)

            # Step 3: åŒ¹é…æ™‚é–“è»¸
            timeline_rules = perform_sliding_window_match(asr_words_data, replacement_map)
            
            with st.expander("æŸ¥çœ‹ AI æ›¿æ›é‚è¼¯ç´°ç¯€"):
                st.write(timeline_rules)

            # Step 4: TTS ç”Ÿæˆ (OpenAI)
            status_text.text("æ­£åœ¨ç”Ÿæˆç”œç¾çš„è²éŸ³ (TTS)...")
            tts_files = {}
            for idx, rule in enumerate(timeline_rules):
                speed = 1.0
                if rule['speed_prompt'] == 'fast': speed = 1.2
                elif rule['speed_prompt'] == 'slow': speed = 0.8
                
                resp = openai_client.audio.speech.create(
                    model="tts-1", voice="nova", input=rule['replacement'], speed=speed
                )
                tts_files[f"replacement_{idx}"] = (f"rep_{idx}.mp3", io.BytesIO(resp.content), "audio/mpeg")
            
            # å¡«è£œç©ºç¼º (Padding)
            for i in range(5):
                key = f"replacement_{i}"
                if key not in tts_files:
                    tts_files[key] = ('dummy.bin', io.BytesIO(b'dummy'), 'application/octet-stream')
            
            progress_bar.progress(70)

            # Step 5: æ··éŸ³è«‹æ±‚
            status_text.text("æ­£åœ¨é€²è¡Œæœ€çµ‚é­”æ³•åˆæˆ...")
            audio_input.seek(0)
            
            censor_rules_json = json.dumps([{
                "replacement": r['replacement'],
                "start_time": r['start_time'],
                "end_time": r['end_time']
            } for r in timeline_rules])

            # æ³¨æ„ï¼šst.audio_input çš„ name å±¬æ€§å¯èƒ½ä¸å›ºå®šï¼Œæˆ‘å€‘æ‰‹å‹•çµ¦ä¸€å€‹
            original_filename = "recording.wav" 
            
            files_to_upload = {
                'original_audio': (original_filename, audio_input, "audio/wav"),
                **tts_files
            }
            
            mix_response = requests.post(
                EXTERNAL_MIX_URL,
                data={'censor_rules': censor_rules_json},
                files=files_to_upload
            )

            if mix_response.status_code == 200:
                progress_bar.progress(100)
                status_text.text("âœ¨ å®Œæˆï¼")
                st.balloons() # æ”¾å€‹æ…¶ç¥ç‰¹æ•ˆ
                
                st.subheader("ğŸ§ æ‚¨çš„æ·¨åŒ–ç‰ˆèªéŸ³")
                st.audio(mix_response.content, format='audio/mpeg')
                
                st.download_button(
                    label="ä¸‹è¼‰ MP3",
                    data=mix_response.content,
                    file_name="censored_recording.mp3",
                    mime="audio/mpeg"
                )
            else:
                st.error(f"æ··éŸ³æœå‹™ç™¼ç”ŸéŒ¯èª¤: {mix_response.text}")

        except Exception as e:
            st.error(f"ç™¼ç”Ÿé æœŸå¤–çš„éŒ¯èª¤: {str(e)}")
            import traceback
            st.code(traceback.format_exc())
