import streamlit as st
import os
import json
import io
import requests
import tempfile
from google.cloud import speech
from google.oauth2 import service_account
from google import genai
from google.genai import types
from openai import OpenAI

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="AI èªéŸ³æ·¨åŒ–å™¨", page_icon="âœ¨")
st.title("âœ¨ AI èªéŸ³æƒ…ç·’æ·¨åŒ–å™¨")
st.markdown("ä¸Šå‚³ä¸€æ®µèªéŸ³ï¼ŒAI å°‡è‡ªå‹•è­˜åˆ¥è² é¢è©å½™ä¸¦æ›¿æ›ç‚ºç¾å¥½çš„è©èªã€‚")

# --- å´é‚Šæ¬„ï¼šAPI è¨­å®š (æœ¬åœ°æ¸¬è©¦ç”¨ .envï¼Œé›²ç«¯ç”¨ st.secrets) ---
# ç‚ºäº†æ–¹ä¾¿éƒ¨ç½²ï¼Œæˆ‘å€‘å„ªå…ˆæª¢æŸ¥ st.secretsï¼Œå¦‚æœæ²’æœ‰å‰‡å˜—è©¦ç’°å¢ƒè®Šæ•¸
# å®šç¾©ä¸€å€‹è®€å–é‡‘é‘°çš„å‡½æ•¸ï¼Œå„ªå…ˆæŸ¥ Secretsï¼Œæ²’æœ‰æ‰æŸ¥ç³»çµ±è®Šæ•¸
def get_secret(key):
    if key in st.secrets:
        return st.secrets[key]
    if os.getenv(key):
        return os.getenv(key)
    return None # æˆ–æ˜¯æ‹‹å‡ºéŒ¯èª¤

# ç²å–é‡‘é‘°
google_api_key = get_secret("GOOGLE_API_KEY")
openai_api_key = get_secret("OPENAI_API_KEY")

# æª¢æŸ¥æ˜¯å¦æˆåŠŸç²å– (é€™ä¸€æ­¥å¾ˆé‡è¦ï¼Œå¯ä»¥é¿å…å ±å‡ºé›£æ‡‚çš„éŒ¯èª¤)
if not google_api_key:
    st.error("æ‰¾ä¸åˆ° GOOGLE_API_KEYï¼Œè«‹æª¢æŸ¥ Secrets è¨­å®šã€‚")
    st.stop()

if not openai_api_key:
    st.error("æ‰¾ä¸åˆ° OPENAI_API_KEYï¼Œè«‹æª¢æŸ¥ Secrets è¨­å®šã€‚")
    st.stop()

# åˆå§‹åŒ– Client
gemini_client = genai.Client(api_key=google_api_key)
openai_client = OpenAI(api_key=openai_api_key)

# --- æ ¸å¿ƒé‚è¼¯å‡½æ•¸ (å¿«å–ä»¥æå‡æ•ˆèƒ½) ---
# 1. åˆå§‹åŒ– Google ASR Client
@st.cache_resource
def get_speech_client():
    # å˜—è©¦å¾ secrets è®€å– Google Cloud JSON å…§å®¹
    if "google_cloud" in st.secrets:
        # å°‡ secrets è½‰æ›ç‚º dict
        creds_dict = dict(st.secrets["google_cloud"])
        creds = service_account.Credentials.from_service_account_info(creds_dict)
        return speech.SpeechClient(credentials=creds)
    else:
        # æœ¬åœ°é–‹ç™¼å¦‚æœè¨­å®šäº†ç’°å¢ƒè®Šæ•¸è·¯å¾‘
        return speech.SpeechClient()

# 2. åˆå§‹åŒ–å…¶ä»– Clients
try:
    speech_client = get_speech_client()
    gemini_client = genai.Client(api_key=get_secret("GOOGLE_API_KEY"))
    openai_client = OpenAI(api_key=get_secret("OPENAI_API_KEY"))
except Exception as e:
    st.error(f"API åˆå§‹åŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥ Secrets è¨­å®š: {e}")
    st.stop()

EXTERNAL_MIX_URL = "https://a67e4a6a0969.ngrok-free.app/mix"

# --- è¼”åŠ©å‡½æ•¸ï¼šæ»‘å‹•è¦–çª—åŒ¹é… ---
def perform_sliding_window_match(asr_words: list, replacement_map: dict) -> list:
    final_logs = []
    i = 0
    n = len(asr_words)
    MAX_WINDOW_SIZE = 5

    while i < n:
        matched = False
        for window_size in range(min(MAX_WINDOW_SIZE, n - i), 0, -1):
            words_slice = asr_words[i : i + window_size]
            candidate_phrase = "".join([w['word'] for w in words_slice])
            
            if candidate_phrase in replacement_map:
                replacement_word = replacement_map[candidate_phrase]
                start_seconds = words_slice[0]['start_time'].total_seconds()
                end_seconds = words_slice[-1]['end_time'].total_seconds()
                
                duration = end_seconds - start_seconds + 1.5
                if duration <= 0: duration = 0.5
                
                speed_instruction = "normal"
                if duration < 0.4: speed_instruction = "fast"
                elif duration > 1.5: speed_instruction = "slow"

                final_logs.append({
                    "original_word": candidate_phrase,
                    "replacement": replacement_word,
                    "start_time": f"{start_seconds}s",
                    "end_time": f"{end_seconds}s",
                    "duration_seconds": duration,
                    "speed_prompt": speed_instruction
                })
                i += window_size
                matched = True
                break
        if not matched:
            i += 1
    return final_logs

# --- ä¸»ä»‹é¢ ---
uploaded_file = st.file_uploader("è«‹é¸æ“‡éŸ³è¨Šæª”æ¡ˆ (WAV, MP3, WEBM)", type=["wav", "mp3", "webm", "m4a"])

if uploaded_file is not None:
    st.audio(uploaded_file, format='audio/audio', start_time=0)
    
    if st.button("ğŸš€ é–‹å§‹è½‰æ›", type="primary"):
        status_text = st.empty()
        progress_bar = st.progress(0)

        try:
            # Step 1: è®€å–æª”æ¡ˆèˆ‡ ASR
            status_text.text("æ­£åœ¨é€²è¡ŒèªéŸ³è­˜åˆ¥ (ASR)...")
            progress_bar.progress(10)
            
            audio_content = uploaded_file.read()
            audio = speech.RecognitionAudio(content=audio_content)
            config = speech.RecognitionConfig(
                encoding=speech.RecognitionConfig.AudioEncoding.WEBM_OPUS, # è‹¥æª”æ¡ˆæ ¼å¼ä¸åŒéœ€èª¿æ•´ï¼Œæˆ–ä½¿ç”¨ ENCODING_UNSPECIFIED
                sample_rate_hertz=48000,
                language_code="zh-TW",
                enable_word_time_offsets=True,
                enable_automatic_punctuation=True,
            )

            operation = speech_client.recognize(config=config, audio=audio)
            
            if not operation.results:
                st.error("ç„¡æ³•è­˜åˆ¥èªéŸ³ï¼Œè«‹ç¢ºèªéŸ³è¨Šæ¸…æ™°åº¦ã€‚")
                st.stop()

            result = operation.results[0].alternatives[0]
            transcript = result.transcript
            
            # æ•´ç† ASR æ•¸æ“š
            asr_words_data = []
            for word_info in result.words:
                asr_words_data.append({
                    "word": word_info.word.strip(),
                    "start_time": word_info.start_time,
                    "end_time": word_info.end_time
                })
            
            st.info(f"è­˜åˆ¥æ–‡æœ¬: {transcript}")
            progress_bar.progress(30)

            # Step 2: LLM åˆ¤æ–·
            status_text.text("AI æ­£åœ¨å¯©æŸ¥æƒ…ç·’è©å½™...")
            
            schema = {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "original_word": {"type": "string"},
                        "replacement_word": {"type": "string"}
                    },
                    "required": ["original_word", "replacement_word"]
                }
            }
            
            prompt = f"""
            ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æƒ…ç·’è©å½™å¯©æŸ¥èˆ‡è½‰æ›å¼•æ“ã€‚
            ä»»å‹™ï¼šæ‰¾å‡ºè² é¢æƒ…ç·’è©å½™ä¸¦æ›¿æ›ç‚ºæ­£å‘ã€æ„è±¡ç¾å¥½çš„è©å½™ (å¦‚ï¼šå½©è™¹ã€èŠ±æœµã€æ³¡æ³¡)ã€‚
            è¼¸å…¥æ–‡æœ¬: "{transcript}"
            """
            
            llm_response = gemini_client.models.generate_content(
                model='gemini-2.0-flash',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema=schema,
                )
            )
            censor_list = json.loads(llm_response.text)
            replacement_map = { item['original_word'].strip(): item['replacement_word'] for item in censor_list }
            
            if not replacement_map:
                st.success("æ²’æœ‰æª¢æ¸¬åˆ°è² é¢è©å½™ï¼")
                st.stop()
                
            progress_bar.progress(50)

            # Step 3: åŒ¹é…æ™‚é–“è»¸
            timeline_rules = perform_sliding_window_match(asr_words_data, replacement_map)
            st.write("æ›¿æ›è¨ˆåŠƒ:", timeline_rules)

            # Step 4: TTS ç”Ÿæˆ
            status_text.text("æ­£åœ¨ç”Ÿæˆæ›¿æ›éŸ³è¨Š (TTS)...")
            tts_files = {}
            for idx, rule in enumerate(timeline_rules):
                speed = 1.0
                if rule['speed_prompt'] == 'fast': speed = 1.2
                elif rule['speed_prompt'] == 'slow': speed = 0.8
                
                resp = openai_client.audio.speech.create(
                    model="tts-1", voice="nova", input=rule['replacement'], speed=speed
                )
                tts_files[f"replacement_{idx}"] = (f"rep_{idx}.mp3", io.BytesIO(resp.content), "audio/mpeg")
            
            # Padding
            for i in range(5):
                key = f"replacement_{i}"
                if key not in tts_files:
                    tts_files[key] = ('dummy.bin', io.BytesIO(b'dummy'), 'application/octet-stream')
            
            progress_bar.progress(70)

            # Step 5: æ··éŸ³
            status_text.text("æ­£åœ¨é€²è¡Œæœ€çµ‚æ··éŸ³...")
            uploaded_file.seek(0)
            
            censor_rules_json = json.dumps([{
                "replacement": r['replacement'],
                "start_time": r['start_time'],
                "end_time": r['end_time']
            } for r in timeline_rules])

            files_to_upload = {
                'original_audio': (uploaded_file.name, uploaded_file, uploaded_file.type),
                **tts_files
            }
            
            mix_response = requests.post(
                EXTERNAL_MIX_URL,
                data={'censor_rules': censor_rules_json},
                files=files_to_upload
            )

            if mix_response.status_code == 200:
                progress_bar.progress(100)
                status_text.text("å®Œæˆï¼")
                st.success("è½‰æ›æˆåŠŸï¼")
                
                # å±•ç¤ºèˆ‡ä¸‹è¼‰
                st.audio(mix_response.content, format='audio/mpeg')
                st.download_button(
                    label="ä¸‹è¼‰è™•ç†å¾Œçš„éŸ³è¨Š",
                    data=mix_response.content,
                    file_name="censored_audio.mp3",
                    mime="audio/mpeg"
                )
            else:
                st.error(f"æ··éŸ³æœå‹™éŒ¯èª¤: {mix_response.text}")

        except Exception as e:
            st.error(f"ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
